{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cRzlrxVvmICpeyc1DSG7iPnGusurmVZs","timestamp":1664307082972}],"mount_file_id":"1mPD9G8RePWJsuOKF9MmHOoEzBQPllS8B","authorship_tag":"ABX9TyOqDVj7QttZ+BkEQRjsLTag"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["\n","**cycleGAN model**\n","\n","Based on the code by Jason Brownlee from his blogs on https://machinelearningmastery.com/\n","I am adapting his code to various applications but original credit goes to Jason. \n","\n","The model uses instance normalization layer:\n","Normalize the activations of the previous layer at each step,\n","i.e. applies a transformation that maintains the mean activation\n","close to 0 and the activation standard deviation close to 1.\n","Standardizes values on each output feature map rather than across features in a batch. ​\n","\n","Download instance normalization code from here: \n","\n","https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/normalization/instancenormalization.py\n","\n","Or install keras_contrib using guidelines here: \n","\n","https://github.com/keras-team/keras-contrib \n"],"metadata":{"id":"NBdhDhkxXfgQ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ilqbDFscv55L"}},{"cell_type":"code","source":["!pip install tensorflow==2.4.1\n","#for installing \n","!pip install keras==2.3.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4VPiXLxlxobm","outputId":"7bf51242-9d88-4936-a97d-a58a622b5b24","executionInfo":{"status":"ok","timestamp":1665426245043,"user_tz":-330,"elapsed":6486,"user":{"displayName":"DAKSHI GOEL","userId":"04553446645191165615"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow==2.4.1 in /usr/local/lib/python3.7/dist-packages (2.4.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.3.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.17.3)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.7.4.3)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.8.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.4.0)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.32.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.19.5)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.37.1)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.15.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.10.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.12.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (5.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.1)\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras "],"metadata":{"id":"fIopt0Y1xzMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8r3M8khkWuOC"},"outputs":[],"source":["from random import random\n","from numpy import load\n","from numpy import zeros\n","from numpy import ones\n","from numpy import asarray\n","from numpy.random import randint\n","from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Adam\n","from tensorflow.keras.initializers import RandomNormal\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","#from tensorflow.keras.models import Input\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Conv2DTranspose\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Concatenate\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","source":["#!pip install git+https://www.github.com/keras-team/keras-contrib.git\n","#from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization"],"metadata":{"id":"-dxzBwQxakZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Cyclegans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARZMhmIzcF1I","executionInfo":{"status":"ok","timestamp":1665426274844,"user_tz":-330,"elapsed":3,"user":{"displayName":"DAKSHI GOEL","userId":"04553446645191165615"}},"outputId":"a34d81d0-bd8a-4b3d-a1b2-cb802ee6f50c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Cyclegans\n"]}]},{"cell_type":"code","source":["from instancenorm import InstanceNormalization"],"metadata":{"id":"5Vzfs3DZcUgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import pyplot"],"metadata":{"id":"HK2to0qvZPuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# discriminator model (70x70 patchGAN)\n","# C64-C128-C256-C512\n","#After the last layer, conv to 1-dimensional output, followed by a Sigmoid function.  \n","# The “axis” argument is set to -1 for instance norm. to ensure that features are normalized per feature map.\n","def define_discriminator(image_shape):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# source image input\n","\tin_image = Input(shape=image_shape)\n","\t# C64: 4x4 kernel Stride 2x2\n","\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# C128: 4x4 kernel Stride 2x2\n","\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","\td = InstanceNormalization(axis=-1)(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# C256: 4x4 kernel Stride 2x2\n","\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","\td = InstanceNormalization(axis=-1)(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# C512: 4x4 kernel Stride 2x2 \n","    # Not in the original paper. Comment this block if you want.\n","\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","\td = InstanceNormalization(axis=-1)(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# second last output layer : 4x4 kernel but Stride 1x1\n","\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n","\td = InstanceNormalization(axis=-1)(d)\n","\td = LeakyReLU(alpha=0.2)(d)\n","\t# patch output\n","\tpatch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n","\t# define model\n","\tmodel = Model(in_image, patch_out)\n","\t# compile model\n","    #The model is trained with a batch size of one image and Adam opt. \n","    #with a small learning rate and 0.5 beta. \n","    #The loss for the discriminator is weighted by 50% for each model update.\n","    #This slows down changes to the discriminator relative to the generator model during training.\n","\tmodel.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n","\treturn model"],"metadata":{"id":"2JK1WIxWchIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generator a resnet block to be used in the generator\n","# residual block that contains two 3 × 3 convolutional layers with the same number of filters on both layers.\n","def resnet_block(n_filters, input_layer):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# first convolutional layer\n","\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\tg = Activation('relu')(g)\n","\t# second convolutional layer\n","\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\t# concatenate merge channel-wise with input layer\n","\tg = Concatenate()([g, input_layer])\n","\treturn g"],"metadata":{"id":"LMdMYt6hcl8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the  generator model - encoder-decoder type architecture\n","\n","#c7s1-k denote a 7×7 Convolution-InstanceNorm-ReLU layer with k filters and stride 1. \n","#dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2.\n","# Rk denotes a residual block that contains two 3 × 3 convolutional layers\n","# uk denotes a 3 × 3 fractional-strided-Convolution InstanceNorm-ReLU layer with k filters and stride 1/2\n","\n","#The network with 6 residual blocks consists of:\n","#c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,u128,u64,c7s1-3\n","\n","#The network with 9 residual blocks consists of:\n","#c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,R256,R256,R256,u128, u64,c7s1-3\n","\n","def define_generator(image_shape, n_resnet=9):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# image input\n","\tin_image = Input(shape=image_shape)\n","\t# c7s1-64\n","\tg = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\tg = Activation('relu')(g)\n","\t# d128\n","\tg = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\tg = Activation('relu')(g)\n","\t# d256\n","\tg = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\tg = Activation('relu')(g)\n","\t# R256\n","\tfor _ in range(n_resnet):\n","\t\tg = resnet_block(256, g)\n","\t# u128\n","\tg = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\tg = Activation('relu')(g)\n","\t# u64\n","\tg = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\tg = Activation('relu')(g)\n","\t# c7s1-3\n","\tg = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n","\tg = InstanceNormalization(axis=-1)(g)\n","\tout_image = Activation('tanh')(g)\n","\t# define model\n","\tmodel = Model(in_image, out_image)\n","\treturn model"],"metadata":{"id":"LRvQ4xxAcpUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt"],"metadata":{"id":"7StjdG4HdJKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_loss(real_image,generated_image):\n","  # converting to gray scale\n","  gray = cv2.cvtColor(generated_image[0], cv2.IMREAD_GRAYSCALE)\n","  #gray=rgb2gray(cv2imread(generated_image[0]));\n","  # remove noise\n","  img = cv2.GaussianBlur(gray,(3,3),0)\n","  # convolute with proper kernels\n","  sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)  #x\n","  sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)  #y\n","  #calculating the magnitude and angle\n","  magnitude = np.sqrt(sobelx**2.0 + sobely**2.0)\n","  angle = np.arctan2(sobely, sobelx) * (180 / np.pi)\n","  #Calculate sum of pixel intensities (number of nonzero matrix elements).\n","  sumIntensities = np.sum(np.sum(magnitude))\n","  #Number of pixels, whose intensity value is greater than a threshold in Sobel edge image.\n","  numberEdgels = np.count_nonzero(magnitude)\n","  #Calculate Entropy of enhanced image.\n","  p = cv2.calcHist([img],[0],None,[256],[0,256])\n","  entropy = -np.sum(np.dot(p[0],np.log2(p[0])))\n","  #Compute objective function which tells us about the quality of the input enhanced image.\n","  a = np.log(np.log(sumIntensities))\n","  b = numberEdgels/np.dot(img[0], img[1])\n","  oFit = np.maximum(np.dot(np.dot(a, b), entropy))\n","  \n","  return oFit"],"metadata":{"id":"tUdhS8yKCgNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a composite model for updating generators by adversarial and cycle loss\n","#We define a composite model that will be used to train each generator separately. \n","def define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n","\t# Make the generator of interest trainable as we will be updating these weights.\n","    #by keeping other models constant.\n","    #Remember that we use this same function to train both generators,\n","    #one generator at a time. \n","\tg_model_1.trainable = True\n","\t# mark discriminator and second generator as non-trainable\n","\td_model.trainable = False\n","\tg_model_2.trainable = False\n","    \n","\t# adversarial loss\n","\tinput_gen = Input(shape=image_shape)\n","\tgen1_out = g_model_1(input_gen)\n","\toutput_d = d_model(gen1_out)\n","\t# identity loss\n","\tinput_id = Input(shape=image_shape)\n","\toutput_id = g_model_1(input_id)\n","\t# cycle loss - forward\n","\toutput_f = g_model_2(gen1_out)\n","\t# cycle loss - backward\n","\tgen2_out = g_model_2(input_id)\n","\toutput_b = g_model_1(gen2_out)\n","    \n","\t# define model graph\n","\tmodel = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n","\t\n","  # define the optimizer\n","\topt = Adam(lr=0.0002, beta_1=0.5)\n","\t# compile model with weighting of least squares loss and L1 loss\n","\tmodel.compile(loss=[custom_loss, 'mae', 'mae', 'mae'], \n","               loss_weights=[1, 5, 10, 10], optimizer=opt)\n","\treturn model"],"metadata":{"id":"FAmW5Ly1cze0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load and prepare training images\n","def load_real_samples(filename):\n","\t# load the dataset\n","\tdata = load(filename)\n","\t# unpack arrays\n","\tX1, X2 = data['arr_0'], data['arr_1']\n","\t# scale from [0,255] to [-1,1]\n","\tX1 = (X1 - 127.5) / 127.5\n","\tX2 = (X2 - 127.5) / 127.5\n","\treturn [X1, X2]"],"metadata":{"id":"cabXxRJ8c9zR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select a batch of random samples, returns images and target\n","#Remember that for real images the label (y) is 1. \n","def generate_real_samples(dataset, n_samples, patch_shape):\n","\t# choose random instances\n","\tix = randint(0, dataset.shape[0], n_samples)\n","\t# retrieve selected images\n","\tX = dataset[ix]\n","\t# generate 'real' class labels (1)\n","\ty = ones((n_samples, patch_shape, patch_shape, 1))\n","\treturn X, y\n"],"metadata":{"id":"8OdGY_lRej1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate a batch of images, returns images and targets\n","#Remember that for fake images the label (y) is 0. \n","def generate_fake_samples(g_model, dataset, patch_shape):\n","\t# generate fake images\n","\tX = g_model.predict(dataset)\n","\t# create 'fake' class labels (0)\n","\ty = zeros((len(X), patch_shape, patch_shape, 1))\n","\treturn X, y"],"metadata":{"id":"uFv6tgCoelm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# periodically save the generator models to file\n","def save_models(step, g_model_AtoB, g_model_BtoA):\n","\t# save the first generator model\n","\tfilename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n","\tg_model_AtoB.save(filename1)\n","\t# save the second generator model\n","\tfilename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n","\tg_model_BtoA.save(filename2)\n","\tprint('>Saved: %s and %s' % (filename1, filename2))\n"],"metadata":{"id":"GpDKIfJ3ensW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# periodically generate images using the save model and plot input and output images\n","def summarize_performance(step, g_model, trainX, name, n_samples=5):\n","\t# select a sample of input images\n","\tX_in, _ = generate_real_samples(trainX, n_samples, 0)\n","\t# generate translated images\n","\tX_out, _ = generate_fake_samples(g_model, X_in, 0)\n","\t# scale all pixels from [-1,1] to [0,1]\n","\tX_in = (X_in + 1) / 2.0\n","\tX_out = (X_out + 1) / 2.0\n","\t# plot real images\n","\tfor i in range(n_samples):\n","\t\tpyplot.subplot(2, n_samples, 1 + i)\n","\t\tpyplot.axis('off')\n","\t\tpyplot.imshow(X_in[i])\n","\t# plot translated image\n","\tfor i in range(n_samples):\n","\t\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n","\t\tpyplot.axis('off')\n","\t\tpyplot.imshow(X_out[i])\n","\t# save plot to file\n","\tfilename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n","\tpyplot.savefig(filename1)\n","\tpyplot.close()"],"metadata":{"id":"OnPNG-VNepc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# update image pool for fake images to reduce model oscillation\n","# update discriminators using a history of generated images \n","#rather than the ones produced by the latest generators.\n","#Original paper recommended keeping an image buffer that stores \n","#the 50 previously created images.\n","\n","def update_image_pool(pool, images, max_size=50):\n","\tselected = list()\n","\tfor image in images:\n","\t\tif len(pool) < max_size:\n","\t\t\t# stock the pool\n","\t\t\tpool.append(image)\n","\t\t\tselected.append(image)\n","\t\telif random() < 0.5:\n","\t\t\t# use image, but don't add it to the pool\n","\t\t\tselected.append(image)\n","\t\telse:\n","\t\t\t# replace an existing image and use replaced image\n","\t\t\tix = randint(0, len(pool))\n","\t\t\tselected.append(pool[ix])\n","\t\t\tpool[ix] = image\n","\treturn asarray(selected)"],"metadata":{"id":"XhVK5BYhetuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# train cyclegan models\n","def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset, epochs=1):\n","\t# define properties of the training run\n","\tn_epochs, n_batch, = epochs, 1  #batch size fixed to 1 as suggested in the paper\n","\t# determine the output square shape of the discriminator\n","\tn_patch = d_model_A.output_shape[1]\n","\t# unpack dataset\n","\ttrainA, trainB = dataset\n","\t# prepare image pool for fake images\n","\tpoolA, poolB = list(), list()\n","\t# calculate the number of batches per training epoch\n","\tbat_per_epo = int(len(trainA) / n_batch)\n","\t# calculate the number of training iterations\n","\tn_steps = bat_per_epo * n_epochs\n","    \n","\t# manually enumerate epochs\n","\tfor i in range(n_steps):\n","\t\t# select a batch of real samples from each domain (A and B)\n","\t\tX_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n","\t\tX_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n","\t\t# generate a batch of fake samples using both B to A and A to B generators.\n","\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n","\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n","\t\t# update fake images in the pool. Remember that the paper suggstes a buffer of 50 images\n","\t\tX_fakeA = update_image_pool(poolA, X_fakeA)\n","\t\tX_fakeB = update_image_pool(poolB, X_fakeB)\n","        \n","\t\t# update generator B->A via the composite model\n","\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n","\t\t# update discriminator for A -> [real/fake]\n","\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n","\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n","\t\t\n","    # update generator A->B via the composite model\n","\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n","\t\t# update discriminator for B -> [real/fake]\n","\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n","\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n","\t\t\n","        # summarize performance\n","        #Since our batch size =1, the number of iterations would be same as the size of our dataset.\n","        #In one epoch you'd have iterations equal to the number of images.\n","        #If you have 100 images then 1 epoch would be 100 iterations\n","\t\tprint('Iteration>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n","\t\t# evaluate the model performance periodically\n","        #If batch size (total images)=100, performance will be summarized after every 75th iteration.\n","\t\tif (i+1) % (bat_per_epo * 1) == 0:\n","\t\t\t# plot A->B translation\n","\t\t\tsummarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n","\t\t\t# plot B->A translation\n","\t\t\tsummarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n","\t\tif (i+1) % (bat_per_epo * 5) == 0:\n","\t\t\t# save the models\n","            # #If batch size (total images)=100, model will be saved after \n","            #every 75th iteration x 5 = 375 iterations.\n","\t\t\tsave_models(i, g_model_AtoB, g_model_BtoA)"],"metadata":{"id":"I_irmVtHez50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ezBZjrSmgtx2"},"execution_count":null,"outputs":[]}]}